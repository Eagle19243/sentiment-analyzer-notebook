{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji\n",
    "# !pip install nltk\n",
    "# !pip install pandas\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "from emoji import get_emoji_regexp\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You might know me from some of my GME posts several weeks ago, or from my Planet Money interview. I've been pretty quite the last few weeks and I have a reason why.     First to answer some well asked questions.  I only played shares.    I rolled several of my shares on peaks from Dec-Jan   Yes I made $4million and I'm still holding a lot of shares     Now to what I really wanted to state to new WSB members and old members like myself.     !!!A word of caution!!!   Several people, people pretending to be normal WSB members and then after a few reddit messages informing me they have been in finance for 25 years (going as far as creating new accounts to try and talk to me) Others who have been trying to identify me on Reddit, and asking for me to either email them or call them, and even meet up. Dont engage these people  MY message to these people. Do you think I'm stupid? Anonymity is my friend at this point. We all have seen what happens to users like DFV once you get a name and a face.  Stop trying to threaten people or identify then for money.   Yes I made money. Deal with it. No need to try and scare and threaten someone on the internet. No one in my life other than my wife knows about the money anyway.   Stay safe kids, the internet is a big place. Donâ€™t give out too much details about yourself. Remember, people can have the ability to see your post history on other subreddits if you dont disable it.  Finally: BRO, I just like the stock  I'M STILL HOLDING, and ITM.âœ‹ðŸ’ŽðŸ’ŽðŸ’ŽðŸ¤š   my opinion is that GME is a ðŸš€ waiting to lift off again.    Edit: I went quiet a few weeks ago because of some of the crazy shit people said to me. You name it, it was said. Scams, help me, iâ€™ll find you and kill you. Iâ€™ve never had that attention before. Plus with the DFV legal issues. Yes I also updated my messages so it doesnâ€™t happen again.   Stay safe kids, people are shockingly crazy.\n"
     ]
    }
   ],
   "source": [
    "text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ticker': 'GME', 'sentiment_score': 0.0}, {'ticker': 'A', 'sentiment_score': 0.0}, {'ticker': 'M', 'sentiment_score': 0.3612}]\n"
     ]
    }
   ],
   "source": [
    "# Load tickers\n",
    "df = pd.read_csv('./cleaned_tickers.csv')\n",
    "tickers = df['ticker'].tolist()\n",
    "\n",
    "# Remove emojis if exists\n",
    "text = get_emoji_regexp().sub(u'', text)\n",
    "text = re.sub(r\"and|or\", \".\", text)\n",
    "tokenized_str = sent_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "nlp = en_core_web_sm.load()\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "tokens_without_sw = [word for word in tokenized_str if word not in all_stopwords]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = ([lemmatizer.lemmatize(word) for word in tokens_without_sw])\n",
    "cleaned_output = lemmatized_tokens\n",
    "\n",
    "# Apply a sentiment analyzer\n",
    "sia = SIA()\n",
    "result = dict()\n",
    "\n",
    "for sentence in cleaned_output:\n",
    "    pol_score = sia.polarity_scores(sentence)\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    ticker = None\n",
    "    \n",
    "    for word in words:\n",
    "        if word in tickers:\n",
    "            ticker = word\n",
    "\n",
    "    if not ticker:\n",
    "        continue\n",
    "    \n",
    "    if ticker in result:\n",
    "        result[ticker] = pol_score['compound'] if pol_score['compound'] > result[ticker] else result[ticker]\n",
    "    else:\n",
    "        result[ticker] = pol_score['compound']\n",
    "    \n",
    "data = []\n",
    "for ticker, sentiment_score in result.items():\n",
    "    data.append({\n",
    "        'ticker': ticker,\n",
    "        'sentiment_score': sentiment_score\n",
    "    })\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

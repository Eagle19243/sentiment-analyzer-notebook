{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji\n",
    "# !pip install nltk\n",
    "# !pip install pandas\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "from emoji import get_emoji_regexp\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"TSLA is going to the moon. I think TSLA is the greatest company ever and GM and other car manufacturers don't stand a chance when competing with TSLA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TSLA', 'is', 'going', 'to', 'the', 'moon', 'I', 'think', 'TSLA', 'is', 'the', 'greatest', 'company', 'ever', 'and', 'GM', 'and', 'other', 'car', 'manufacturers', 'don', 't', 'stand', 'a', 'chance', 'when', 'competing', 'with', 'TSLA']\n",
      "{'each', 'as', 'last', 'who', '‘s', 'against', 'n’t', 'is', 'put', 'can', 'up', 'been', 'everyone', 'the', 'do', 'really', 'three', 'thru', 'twelve', 'are', 'anyone', '’ll', 'whole', 'yourselves', 'me', 'seeming', 'about', \"n't\", 'nevertheless', 'four', 'and', '‘ll', 'during', 'or', 'ours', 'several', 'than', 'on', 'themselves', 'yet', 'enough', 'while', 'does', 'few', \"'ve\", 'take', 'first', 'else', 'to', 'something', 'your', 'same', 'many', 'became', 'least', 'go', 'since', 'quite', 'seem', 'others', 'whose', 'once', 'within', 'along', 'thus', 'be', 'it', '‘m', 'amongst', 'somehow', '‘d', 'two', 'myself', 'unless', 'even', 'beyond', 'doing', 'upon', 'for', 'hence', 'twenty', 'hereby', 'former', 'nothing', 'still', 'please', 'so', 'itself', 'what', 'more', 're', 'himself', 'bottom', 'a', 'much', \"'m\", 'when', 'meanwhile', 'being', 'where', 'latter', 'ten', 'get', 'our', 'never', 'whither', '’s', 'n‘t', 'five', 'anyway', 'no', 'through', 'them', 'via', 'how', 'further', 'whether', 'well', 'due', 'namely', 'i', 'amount', 'sixty', 'either', '’m', 'across', 'six', 'he', '’ve', 'somewhere', 'may', 'hundred', 'above', 'after', 'anything', 'hers', 'whom', 'sometime', 'together', 'fifty', 'wherein', 'per', 'were', 'neither', 'hereupon', 'some', 'give', 'whereupon', 'among', 'such', 'did', 'ca', 'though', 'only', 'you', 'that', 'whence', 'nine', 'done', 'already', 'back', 'one', 'becomes', 'empty', 'every', 'her', 'cannot', 'whatever', 'before', 'out', 'nowhere', 'these', 'however', 'those', 'then', \"'ll\", 'toward', 'without', \"'s\", 'ourselves', 'although', 'again', 'had', 'whenever', 'because', 'therefore', 'often', 'she', 'too', 'both', 'which', 'name', 'his', 'this', 'fifteen', 'whereas', 'am', 'its', 'any', 'forty', 'should', 'thence', 'also', 'therein', 'whoever', 'under', 'everywhere', 'whereby', 'over', 'at', 'him', 'perhaps', 'formerly', 'here', 'thereupon', 'now', 'thereafter', 'all', 'third', 'eight', 'top', 'hereafter', 'with', 'none', 'own', 'regarding', 'yourself', 'next', 'has', 'towards', 'very', 'full', 'herself', 'we', 'say', 'part', 'almost', 'beside', 'but', 'another', 'alone', 'around', 'could', 'someone', 'otherwise', 'keep', 'moreover', 'might', 'of', 'anyhow', 'besides', 'seemed', 'nor', 'was', 'wherever', '’d', 'mine', 'until', 'sometimes', '‘re', 'an', 'would', 'yours', 'indeed', 'less', 'call', 'noone', 'in', 'from', 'show', 'other', 'us', 'between', \"'re\", 'everything', 'side', 'off', 'below', 'anywhere', 'eleven', '‘ve', 'down', 'there', 'their', 'beforehand', 'always', 'will', 'by', 'whereafter', 'latterly', 'rather', 'elsewhere', 'herein', 'they', 'not', 'used', 'onto', 'serious', 'made', 'why', 'seems', \"'d\", 'behind', 'various', 'make', 'move', 'see', 'my', 'throughout', 'front', 'if', 'must', 'thereby', 'most', 'becoming', 'mostly', '’re', 'except', 'into', 'just', 'become', 'nobody', 'have', 'using', 'ever', 'afterwards'}\n",
      "    neg  neu  pos  compound         words\n",
      "0   0.0  1.0  0.0    0.0000          tsla\n",
      "1   0.0  1.0  0.0    0.0000         going\n",
      "2   0.0  1.0  0.0    0.0000          moon\n",
      "3   0.0  1.0  0.0    0.0000         think\n",
      "4   0.0  1.0  0.0    0.0000          tsla\n",
      "5   0.0  0.0  1.0    0.6369      greatest\n",
      "6   0.0  1.0  0.0    0.0000       company\n",
      "7   0.0  1.0  0.0    0.0000            gm\n",
      "8   0.0  1.0  0.0    0.0000           car\n",
      "9   0.0  1.0  0.0    0.0000  manufacturer\n",
      "10  0.0  1.0  0.0    0.0000           don\n",
      "11  0.0  0.0  0.0    0.0000             t\n",
      "12  0.0  1.0  0.0    0.0000         stand\n",
      "13  0.0  0.0  1.0    0.2500        chance\n",
      "14  0.0  1.0  0.0    0.0000     competing\n",
      "15  0.0  1.0  0.0    0.0000          tsla\n"
     ]
    }
   ],
   "source": [
    " # Remove emojis if exists\n",
    "text = get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "# Break apart every word in the string into an individual word\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
    "tokenized_str = tokenizer.tokenize(text)\n",
    "print(tokenized_str)\n",
    "\n",
    "# Convert tokens into lowercase\n",
    "lower_str_tokenized = [word.lower() for word in tokenized_str]\n",
    "\n",
    "# Remove stop words\n",
    "nlp = en_core_web_sm.load()\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "print(all_stopwords)\n",
    "tokens_without_sw = [word for word in lower_str_tokenized if word not in all_stopwords]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = ([lemmatizer.lemmatize(word) for word in tokens_without_sw])\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = ([stemmer.stem(word) for word in tokens_without_sw])\n",
    "cleaned_output = lemmatized_tokens\n",
    "\n",
    "# Apply a sentiment analyzer\n",
    "sia = SIA()\n",
    "results = []\n",
    "\n",
    "for sentences in cleaned_output:\n",
    "    pol_score = sia.polarity_scores(sentences)\n",
    "    pol_score['words'] = sentences\n",
    "    results.append(pol_score)\n",
    "\n",
    "pd.set_option('display.max_columns', None, 'max_colwidth', None)\n",
    "df = pd.DataFrame.from_records(results)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
